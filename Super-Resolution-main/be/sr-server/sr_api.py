import sys
import os

# Add parent directory to Python path
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, BASE_DIR)

# Then add current directory
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, CURRENT_DIR)

# ruff: noqa: E402
import faulthandler
import time
import logging
import threading
import json
import base64
from datetime import datetime, timedelta, timezone
from typing import Dict, Any

from mimetypes import guess_type

import tensorflow_hub as hub

from dotenv import load_dotenv
from supabase import create_client, Client

from celery import Celery
from celery.signals import worker_process_init

from utility import setup_logging, get_new_uuid7, create_pdf



# consts and env
os.environ['KMP_DUPLICATE_LIB_OK']='True'

load_dotenv()

LOGS_DIR = 'logs'
UPLOAD_DIR = 'uploads'
PROCESSED_DIR = 'processed'
REPORT_DIR = 'reports'
REQUEST_INFO_DIR = 'requests'
POOL_SIZE = 2
SAVED_MODEL_PATH = "./esrgan-tf2/1"
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_KEY')
JWT_SECRET = os.getenv('SUPABASE_JWT_SECRET')
utc_plus_4 = timezone(timedelta(hours=4))



# faulthandler
timestamp = int(time.time())
filename = f'{os.path.join(os.path.normpath(LOGS_DIR), str(timestamp))}.log'
with open(filename, "w") as f:
    faulthandler.enable(file=f, all_threads=True)


# app setup
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

logger = logging.getLogger('sr.app')
logger = setup_logging(logger)
logger.info('Starting SR Worker Server.')



# Celery setup inside Internal API
celery = Celery(
    'sr_tasks',
    broker='amqp://guest:guest@192.168.1.7:5672//',  # Connect to RabbitMQ
    backend='rpc://'  # Using RabbitMQ for result backend
)


celery.conf.update(
    task_track_started=True,
    task_time_limit=300,
    worker_concurrency=1,
    worker_prefetch_multiplier=1,  # No prefetching
    worker_max_tasks_per_child=50,  # Prevent memory leaks
    worker_redirect_stdouts=False,
    worker_hijack_root_logger=False,
    broker_connection_retry_on_startup=True,
    task_always_eager=False,
    # imports=['sr_api'],
    worker_proc_alive_timeout=30,
    # worker_max_memory_per_child=250000  # 250MB
)

# Initialize the model instance in each worker process
@worker_process_init.connect
def init_worker(**kwargs):
    global model_instance, model_method
    try: 
        # import sys
        # # Backup the current sys.argv
        # original_argv = sys.argv.copy()
        # # Clear sys.argv so that super_resol doesn't see celery's extra arguments
        # sys.argv = [sys.argv[0]]

        import super_resol

        # # Restore original sys.argv
        # sys.argv = original_argv

        enh_model = hub.load(SAVED_MODEL_PATH)
        model_instance, model_method = enh_model, super_resol.enhance_image_api_method
    except ModuleNotFoundError as e:
        logger.critical("FATAL IMPORT ERROR: %s", e)
        raise





class ProcessingResult():
    def __init__(self):
        self.result = None
        self.error = None
        self.event = threading.Event()

def process_thread(file_path, result):
    logger.info(f'Starting thread for {file_path}')
    try:
        logger.info("Using global model instance")
        processed_images = model_method(file_path, model_instance)
        if processed_images is None:
            result.error = ('Error occurred in image processing '
                            '(No retina found). Please retry. '
                            'If issue persists, contact system administrator.')
        else:
            logger.info('Image processed successfully')
            result.result = processed_images
    except Exception as e:
        result.result = []
        result.error = f'Error in processing. Please try again. {e}'
    logger.info(f'Closing thread for {file_path}')
    result.event.set()


# @app.route('/api/pool_size', methods=['GET'])
# def get_pool_size():
#     return {'pool_size': model_pool.qsize()}


# Celery Task
@celery.task(bind=True, name='sr_tasks.test_task', autoretry_for=(Exception,), max_retries=3)
def test_task(self, *args):
    logger.debug(f"Received args: {args}")
    email, user_id = args[0], args[1]
    fields = []
    # fields.append({'pool_size': model_pool.qsize()})
    fields.append({'email': email})
    fields.append({'user_id': user_id})
    return {'data': fields}


@celery.task(bind=True, name='sr_tasks.process_image_task', autoretry_for=(Exception,), max_retries=3, 
             retry_backoff=True, retry_backoff_max=60, retry_jitter=False)
def process_image_task(self, file_data: bytes, extension: str, user: dict):
    """Process image data in the internal API."""
    try:
        # Generate request ID. TODO: Instead of generating uuid here, use task_id which is also a uuid but generated by rabbitmq
        unixt = time.time()
        taskId = self.request.id

        client_email = user.get('email')
        client_host = user['ip']

        # TODO: Will have to setup database (on network, not local) so that multiple instances of this app (comprised of workers) will be able to save data in one place - maybe object storage (minIO) will be better for this purpose
        # Save request image
        filename = f'{taskId}.{extension}'
        file_path = os.path.join(os.path.normpath(UPLOAD_DIR), filename)
        with open(file_path, 'wb') as f:
            f.write(file_data)
        # file.save(file_path)

        procst = time.time()
        result = ProcessingResult()
        thread = threading.Thread(target=process_thread, args=(file_path, result))
        thread.start()
        result.event.wait()
        procen = time.time()
        proctime = round(procen - procst, 2)
        logger.warning('Proc time: %s', proctime)

        if result.error:
            return {'error': result.error}

        processed_images = result.result

        # Save processed images
        # TODO: Move the save logic to execute async so response is sent back even faster. Will have to convert the processed_images to base64
        processed_images_paths = []
        proccessed_images_dir = os.path.join(os.path.normpath(PROCESSED_DIR), str(taskId))
        os.makedirs(proccessed_images_dir, exist_ok=True)
        for i, img in enumerate(processed_images):
            proccessed_filename = f'{taskId}_{i}.{extension}'
            pathforprocessed = os.path.join(proccessed_images_dir, proccessed_filename)
            processed_images_paths.append(pathforprocessed)
            img.save(pathforprocessed) 

        # Save request info
        requestInfo = {
            "timestamp": str(datetime.fromtimestamp(unixt)),
            "taskId": str(taskId),
            # "userId": user_id,
            "userEmail": client_email,
            # "userLastSignIn": str(user_last_sign_in_at.astimezone(utc_plus_4)),
            "requestFilePath": file_path,
            "requestProcessedImagesPaths": processed_images_paths,
            "ip_address": client_host,
            # "user_agent": request.headers.get('User-Agent'),
        } 
        request_info_filepath = os.path.join(os.path.normpath(REQUEST_INFO_DIR), str(taskId))
        with open(f'{request_info_filepath}.json', 'w') as json_file:
            json.dump(requestInfo, json_file, indent=2)
        logger.info(f'Request info written to {request_info_filepath}')

        # Save pdf
        pdf_data = create_pdf(file_path, processed_images_paths, requestInfo)

        # Generate response 
        # TODO: rewrite to use raw image data not saved images - check how much time can be saved. only do it if time is more than 100ms
        fields = []
        for i, path in enumerate(processed_images_paths):
            if os.path.exists(path):
                with open(path, 'rb') as img_file:
                    img_base64 = base64.b64encode(img_file.read()).decode('utf-8')
                    mimetype = guess_type(path)[0] or 'application/octet-stream'
                    img_data = {
                        'filename': os.path.basename(path),
                        'mimetype': mimetype,
                        'data': img_base64
                    }
                    fields.append(img_data)
            else:
                print(f'Warning: {path} does not exist')
                return {'error': 'Error occured in image processing (No processed images saved on server). Please retry. If issue persists, contact system administrator.'}

        # # Save PDF for test purposes
        pdf_output_path = os.path.join(os.path.normpath(REPORT_DIR), f'{str(taskId)}.pdf')
        # with open(pdf_output_path, 'wb') as f:
        #     f.write(pdf_data)
        # print(f"PDF saved to: {pdf_output_path}")

        # Add PDF to the response
        pdf_base64 = base64.b64encode(pdf_data).decode('utf-8')
        pdf_obj = {
            'filename': os.path.basename(pdf_output_path),
            'mimetype': 'application/pdf',
            'data': pdf_base64
        }
        fields.append(pdf_obj)

        return {'images': fields, 'proc': proctime}
    except Exception as e:
        logger.warning(f"Task failed: {str(e)}")
        return {'error': f'Task failed: {str(e)}'}


logger.info('''

                                            8888888b.                            888          888    d8b                                             d888        .d8888b.  
                                            888   Y88b                           888          888    Y8P                                            d8888       d88P  Y88b 
                                            888    888                           888          888                                                     888       888    888 
.d8888b  888  888 88888b.   .d88b.  888d888 888   d88P .d88b.  .d8888b   .d88b.  888 888  888 888888 888  .d88b.  88888b.                   888  888  888       888    888 
88K      888  888 888 "88b d8P  Y8b 888P"   8888888P" d8P  Y8b 88K      d88""88b 888 888  888 888    888 d88""88b 888 "88b                  888  888  888       888    888 
"Y8888b. 888  888 888  888 88888888 888     888 T88b  88888888 "Y8888b. 888  888 888 888  888 888    888 888  888 888  888      888888      Y88  88P  888       888    888 
     X88 Y88b 888 888 d88P Y8b.     888     888  T88b Y8b.          X88 Y88..88P 888 Y88b 888 Y88b.  888 Y88..88P 888  888                   Y8bd8P   888   d8b Y88b  d88P 
 88888P'  "Y88888 88888P"   "Y8888  888     888   T88b "Y8888   88888P'  "Y88P"  888  "Y88888  "Y888 888  "Y88P"  888  888                    Y88P  8888888 Y8P  "Y8888P"  
                  888                                                                                                                                                      
                  888                                                                                                                                                      
                  888

''')
